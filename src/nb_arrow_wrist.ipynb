{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "from jproperties import Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Properties()\n",
    "with open('../.properties', 'rb') as config_file: configs.load(config_file)\n",
    "CAMERA_INDEX = int(configs.get('CAMERA_INDEX').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('dist') \n",
    "num_videos = 20\n",
    "sequence_length = 60\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear dist\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    os.remove(os.path.join(DATA_PATH, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registraiamo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "width= int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height= int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "width= int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height= int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "for idx_video in range(num_videos):\n",
    "\n",
    "    video_filename = os.path.join(DATA_PATH, f\"{idx_video}.mp4\")\n",
    "    writer= cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'DIVX'), 20, (width,height))\n",
    "\n",
    "    for idx_frame in range(sequence_length):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if idx_frame == 0:\n",
    "            cv2.putText(frame, f'Stating Video #{idx_video+1}', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.imshow('OpenCV Feed', frame)\n",
    "            cv2.waitKey(3000)\n",
    "        else:\n",
    "            cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "        writer.write(frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    writer.release()\n",
    "                \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostriamo e basta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Converti il frame in scala di grigi\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Esegui il rilevamento delle pose con MediaPipe\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Estrai le coordinate della spalla destra, del gomito destro e della mano destra\n",
    "    if results.pose_landmarks:\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "        right_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "        right_wrist = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        # Le coordinate sono normalizzate, quindi convertile in coordinate pixel\n",
    "        height, width, _ = frame.shape\n",
    "        shoulder_x, shoulder_y = int(right_shoulder.x * width), int(right_shoulder.y * height)\n",
    "        elbow_x, elbow_y = int(right_elbow.x * width), int(right_elbow.y * height)\n",
    "        wrist_x, wrist_y = int(right_wrist.x * width), int(right_wrist.y * height)\n",
    "\n",
    "        # Disegna i punti sul frame\n",
    "        cv2.circle(frame, (shoulder_x, shoulder_y), 5, (0, 0, 255), -1)\n",
    "        cv2.circle(frame, (elbow_x, elbow_y), 5, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, (wrist_x, wrist_y), 5, (255, 0, 0), -1)\n",
    "\n",
    "    # Visualizza il frame con le coordinate\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 10\n",
    "X_training = []\n",
    "y_training = []\n",
    "frame_buffer = []\n",
    "video_paths = [os.path.join(DATA_PATH, f\"{idx_video}.mp4\") for idx_video in range(num_videos)]\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            #right_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "            #right_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "            right_wrist = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST]\n",
    "            \n",
    "            # Estrai le coordinate x, y, e z e normalizzale\n",
    "            #shoulder_coords = [right_shoulder.x, right_shoulder.y, right_shoulder.z]\n",
    "            #elbow_coords = [right_elbow.x, right_elbow.y, right_elbow.z]\n",
    "            wrist_coords = [\n",
    "                right_wrist.x, \n",
    "                right_wrist.y, \n",
    "                #right_wrist.z\n",
    "                ]\n",
    "\n",
    "            frame_buffer.append(\n",
    "                # shoulder_coords\n",
    "                #  + elbow_coords \n",
    "                wrist_coords\n",
    "            )\n",
    "\n",
    "            # Se il buffer contiene abbastanza frame, aggiungilo a X_training e y_training\n",
    "            if len(frame_buffer) >= window_len:\n",
    "                X_training.append(frame_buffer[:int(window_len/2)])\n",
    "                y_training.append(frame_buffer[-1])\n",
    "                frame_buffer = frame_buffer[1:]\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = np.array(X_training)\n",
    "y_training = np.array(y_training)\n",
    "X_training.shape, y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with input shape (.., ..)\n",
    "model.add(LSTM(64, input_shape=(X_training.shape[1], X_training.shape[2]), activation='relu', return_sequences=True))\n",
    "\n",
    "# Add another LSTM layer with return_sequences=False for the final prediction\n",
    "model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "\n",
    "# Add a dense output layer with 9 units (assuming you want to predict 9 values)\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_training, y_training, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "frame_buffer = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Converti il frame in scala di grigi\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Esegui il rilevamento delle pose con MediaPipe\n",
    "    results = holistic.process(frame_rgb)\n",
    "\n",
    "    # Estrai le coordinate della spalla destra, del gomito destro e della mano destra\n",
    "    if results.pose_landmarks:\n",
    "        right_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "        right_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "        right_wrist = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        # Le coordinate sono normalizzate, quindi convertile in coordinate pixel\n",
    "        height, width, _ = frame.shape\n",
    "        # shoulder_x, shoulder_y = int(right_shoulder.x * width), int(right_shoulder.y * height)\n",
    "        # elbow_x, elbow_y = int(right_elbow.x * width), int(right_elbow.y * height)\n",
    "        wrist_x, wrist_y = int(\n",
    "            right_wrist.x * width), int(right_wrist.y * height)\n",
    "\n",
    "        # Disegna i punti sul frame\n",
    "        # cv2.circle(frame, (shoulder_x, shoulder_y), 5, (0, 0, 255), -1)\n",
    "        # cv2.circle(frame, (elbow_x, elbow_y), 5, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, (wrist_x, wrist_y), 5, (255, 0, 0), -1)\n",
    "\n",
    "        # Estrai le coordinate x, y, e z e normalizzale\n",
    "        # shoulder_coords = [right_shoulder.x, right_shoulder.y, right_shoulder.z]\n",
    "        # elbow_coords = [right_elbow.x, right_elbow.y, right_elbow.z]\n",
    "        wrist_coords = [\n",
    "            right_wrist.x,\n",
    "            right_wrist.y,\n",
    "            # right_wrist.z\n",
    "        ]\n",
    "        print(wrist_coords)\n",
    "\n",
    "        # Logica per la predizione\n",
    "        frame_buffer.append(\n",
    "            # shoulder_coords\n",
    "            # + elbow_coords\n",
    "            wrist_coords\n",
    "        )\n",
    "\n",
    "        if len(frame_buffer) >= window_len/2:\n",
    "            prediction_input = np.array(frame_buffer)[np.newaxis, :, :]\n",
    "            prediction = model.predict(prediction_input)\n",
    "            print(prediction)\n",
    "\n",
    "            # Draw magenta circle for the predicted wrist position\n",
    "            prediction_x = int(prediction[0][0] * width)\n",
    "            prediction_y = int(prediction[0][1] * height)\n",
    "            cv2.circle(frame, (prediction_x, prediction_y),\n",
    "                       5, (255, 0, 255), -1)\n",
    "            \n",
    "            #Draw an arrow between the predicted and actual wrist position\n",
    "            cv2.arrowedLine(frame, (wrist_x, wrist_y), (prediction_x, prediction_y), (255, 0, 255), 2)\n",
    "\n",
    "            frame_buffer = frame_buffer[1:]\n",
    "\n",
    "    # Visualizza il frame con le coordinate\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uit-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
