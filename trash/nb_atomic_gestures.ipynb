{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import imageio\n",
    "import threading\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "from jproperties import Properties\n",
    "\n",
    "gif_threads = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Properties()\n",
    "with open('../.properties', 'rb') as config_file: configs.load(config_file)\n",
    "CAMERA_INDEX = int(configs.get('CAMERA_INDEX').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp.solutions.holistic.POSE_CONNECTIONS,\n",
    "                             mp.solutions.drawing_utils.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp.solutions.drawing_utils.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTOR_LEN = len(mp.solutions.holistic.PoseLandmark) * 4\n",
    "\n",
    "def extract_descriptor(results):\n",
    "    return np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(DESCRIPTOR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gif(gif_path, window_name, display_time_ms=100):\n",
    "    gif = imageio.get_reader(gif_path)\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    for i, gif_frame in enumerate(gif):\n",
    "        cv2.imshow(window_name, gif_frame)\n",
    "        cv2.waitKey(display_time_ms)  # Display time of each frame in milliseconds\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "# Function to run display_gif in a thread\n",
    "def display_gif_thread(gif_path, window_name):\n",
    "    display_gif(gif_path, window_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, frame):\n",
    "    colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    for idx, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60 + idx * 40), (int(prob * 100), 90 + idx * 40), colors[idx], -1)\n",
    "        cv2.putText(output_frame, actions[idx], (0, 85 + idx * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        print(actions[idx], prob)\n",
    "        if prob > 0.9:\n",
    "            gif_path = os.path.join(DATA_PATH, actions[idx], f'{actions[idx]}.gif')\n",
    "            # Check if a GIF for this action is already running\n",
    "            if actions[idx] not in gif_threads or not gif_threads[actions[idx]].is_alive():\n",
    "                thread = threading.Thread(target=display_gif_thread, args=(gif_path, actions[idx]))\n",
    "                thread.start()\n",
    "                gif_threads[actions[idx]] = thread\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_frame(frame_index, landmarks_list):\n",
    "    plt.clf()\n",
    "    body_landmarks = landmarks_list[frame_index]\n",
    "\n",
    "    # Extract the x and y coordinates of the landmarks\n",
    "    x = [lm.x for lm in body_landmarks.landmark]\n",
    "    y = [lm.y for lm in body_landmarks.landmark]\n",
    "\n",
    "    # Draw landmarks as points\n",
    "    plt.scatter(x, y, s=20, c='blue')\n",
    "\n",
    "    # Draw the connecting lines between the landmarks\n",
    "    connections = [(11, 12), (12, 24), (24, 23), (23, 11),  # Head\n",
    "                   (11, 13), (13, 15), (15, 17),  # Left Arm\n",
    "                   (12, 14), (14, 16), (16, 18),  # Right Arm\n",
    "                   (11, 25), (25, 23),  # Spine\n",
    "                   (24, 26), (26, 28), (28, 30),  # Left Leg\n",
    "                   (23, 27), (27, 29), (29, 31)]  # Right Leg\n",
    "\n",
    "    for connection in connections:\n",
    "        plt.plot([x[connection[0]], x[connection[1]]], [y[connection[0]], y[connection[1]]], c='blue')\n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('Dataset_atomic_gestures') \n",
    "actions = np.array(['hand-up', 'flex', 'idle'])\n",
    "num_sequences = 10\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear dist\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    os.remove(os.path.join(DATA_PATH, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "for action in actions: \n",
    "    if not os.path.exists(os.path.join(DATA_PATH, action)):\n",
    "        os.mkdir(os.path.join(DATA_PATH, action))\n",
    "\n",
    "for action in actions: \n",
    "    for sequence in range(num_sequences):\n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(landmarks_list, action):\n",
    "    fig = plt.figure()\n",
    "    ani = animation.FuncAnimation(fig, partial(draw_frame, landmarks_list=landmarks_list), frames=len(landmarks_list), interval=100)\n",
    "    output_gif = os.path.join(DATA_PATH, action, f'{action}.gif')\n",
    "    ani.save(output_gif, writer='imagemagick', fps=10)\n",
    "    if os.path.exists(output_gif):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        # Flag to check if the GIF has been created for this action\n",
    "        gif_created = False\n",
    "        landmarks_list = []\n",
    "        for sequence in range(num_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Extract body landmarks and append them to the list\n",
    "                if not gif_created:\n",
    "                    body_landmarks = results.pose_landmarks\n",
    "                    if body_landmarks:\n",
    "                        landmarks_list.append(body_landmarks)\n",
    "\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Action: {action}. Video #{sequence}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, f'Action: {action}. Video #{sequence}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                keypoints = extract_descriptor(results)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            # Creation of the .gif\n",
    "            if not gif_created and landmarks_list:\n",
    "                gif_created = create_gif(landmarks_list, action)             \n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int) # one hot encoding\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length, DESCRIPTOR_LEN)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "num_frames_for_stability = 10\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        keypoints = extract_descriptor(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "        \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-num_frames_for_stability:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                if res[np.argmax(res)] > threshold:            \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uit-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
