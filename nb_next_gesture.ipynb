{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import imageio\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "import feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIF_THREADS = {}\n",
    "DESCRIPTOR_LEN = len(mp.solutions.holistic.PoseLandmark) * 4\n",
    "DATA_PATH = os.path.join('dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = np.array(['1', '2', '3'])\n",
    "NUM_VIDEOS_FOR_ACTION = 5\n",
    "NUM_FRAMES_PER_VIDEO = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward.create_folder_if_not_exists(DATA_PATH)\n",
    "\n",
    "for action in ACTIONS:\n",
    "    if not os.path.exists(os.path.join(DATA_PATH, action)):\n",
    "        os.mkdir(os.path.join(DATA_PATH, action))\n",
    "\n",
    "for action in ACTIONS:\n",
    "    for idx_video in range(NUM_VIDEOS_FOR_ACTION):\n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(idx_video)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gif(gif_path: str, window_name: str, display_time_ms: int = 100):\n",
    "    gif = imageio.get_reader(gif_path)\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    for gif_frame in gif:\n",
    "        cv2.imshow(window_name, gif_frame)\n",
    "        # Display time of each frame in milliseconds\n",
    "        cv2.waitKey(display_time_ms)\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "def terminate_all_threads():\n",
    "    global GIF_THREADS\n",
    "    for action, thread in GIF_THREADS.items():\n",
    "        thread.join()\n",
    "    GIF_THREADS = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_suggested_gif(actions, idx):\n",
    "    gif_path = os.path.join(DATA_PATH, actions[idx], f'{actions[idx]}.gif')\n",
    "    # Check if a GIF for this action is already running\n",
    "    if actions[idx] not in GIF_THREADS or not GIF_THREADS[actions[idx]].is_alive():\n",
    "        thread = threading.Thread(target=display_gif, args=(gif_path, actions[idx]))\n",
    "        thread.start()\n",
    "        GIF_THREADS[actions[idx]] = thread\n",
    "\n",
    "def create_gif(landmarks_list, action):\n",
    "    fig = plt.figure()\n",
    "    ani = animation.FuncAnimation(fig, partial(feedforward.draw_frame, landmarks_list=landmarks_list), frames=len(landmarks_list), interval=100)\n",
    "    output_gif = os.path.join(DATA_PATH, action, f'{action}.gif')\n",
    "    ani.save(output_gif, writer='imagemagick', fps=10) # TODO: Capire se si puo usare la variabile NUM_FRAMES_PER_VIDEO\n",
    "    return os.path.exists(output_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(feedforward.CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in ACTIONS:\n",
    "        gif_created = False\n",
    "        landmarks_list = []\n",
    "\n",
    "        for idx_video in range(NUM_VIDEOS_FOR_ACTION):\n",
    "            for frame_num in range(NUM_FRAMES_PER_VIDEO):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = feedforward.mediapipe_detection(frame, holistic)\n",
    "\n",
    "                if not gif_created:\n",
    "                    body_landmarks = results.pose_landmarks\n",
    "                    if body_landmarks:\n",
    "                        landmarks_list.append(body_landmarks)\n",
    "\n",
    "                feedforward.draw_pose_landmarks(image, results)\n",
    "\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Action: {action}. Video #{idx_video}', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, f'Action: {action}. Video #{idx_video}', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(idx_video), str(frame_num))\n",
    "                keypoints = feedforward.extract_descriptor(results, DESCRIPTOR_LEN)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not gif_created and landmarks_list:\n",
    "                gif_created = create_gif(landmarks_list, action)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(ACTIONS)}\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in ACTIONS:\n",
    "    for idx_video in range(NUM_VIDEOS_FOR_ACTION):\n",
    "        window = []\n",
    "        for frame_num in range(NUM_FRAMES_PER_VIDEO):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(idx_video), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0920 - categorical_accuracy: 0.2857\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0458 - categorical_accuracy: 0.5714\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9345 - categorical_accuracy: 0.6429\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8377 - categorical_accuracy: 0.6429\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7675 - categorical_accuracy: 0.6429\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7692 - categorical_accuracy: 0.6429\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7144 - categorical_accuracy: 0.6429\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6895 - categorical_accuracy: 0.7143\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6298 - categorical_accuracy: 0.7857\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5375 - categorical_accuracy: 0.8571\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4757 - categorical_accuracy: 0.7857\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4598 - categorical_accuracy: 0.8571\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5659 - categorical_accuracy: 0.7857\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5159 - categorical_accuracy: 0.7857\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5410 - categorical_accuracy: 0.8571\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5841 - categorical_accuracy: 0.7857\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4617 - categorical_accuracy: 0.8571\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4015 - categorical_accuracy: 0.8571\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3846 - categorical_accuracy: 0.8571\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3876 - categorical_accuracy: 0.7857\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3658 - categorical_accuracy: 0.7857\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4550 - categorical_accuracy: 0.8571\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5095 - categorical_accuracy: 0.7857\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3046 - categorical_accuracy: 0.8571\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3291 - categorical_accuracy: 0.8571\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3386 - categorical_accuracy: 0.8571\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3331 - categorical_accuracy: 0.9286\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3293 - categorical_accuracy: 0.8571\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3054 - categorical_accuracy: 0.8571\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2706 - categorical_accuracy: 0.8571\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3146 - categorical_accuracy: 0.8571\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3430 - categorical_accuracy: 0.8571\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4111 - categorical_accuracy: 0.7857\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3488 - categorical_accuracy: 0.8571\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4073 - categorical_accuracy: 0.8571\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3192 - categorical_accuracy: 0.9286\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3317 - categorical_accuracy: 0.8571\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3022 - categorical_accuracy: 0.8571\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2646 - categorical_accuracy: 0.9286\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2898 - categorical_accuracy: 0.8571\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2372 - categorical_accuracy: 0.9286\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2661 - categorical_accuracy: 0.8571\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2190 - categorical_accuracy: 0.9286\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2494 - categorical_accuracy: 0.8571\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1979 - categorical_accuracy: 0.9286\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2222 - categorical_accuracy: 0.8571\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1900 - categorical_accuracy: 0.9286\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1704 - categorical_accuracy: 0.9286\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1664 - categorical_accuracy: 0.9286\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1537 - categorical_accuracy: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1719fa100>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(NUM_FRAMES_PER_VIDEO, DESCRIPTOR_LEN)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(ACTIONS.shape[0], activation='softmax'))\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 320ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0],\n",
       "        [0, 1]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)\n",
    "#Define precision_score\n",
    "precision_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(yhat)\n",
    "#define recall_score\n",
    "recall_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(ytrue)\n",
    "#Calculate the f score\n",
    "f_score = stats.hmean([precision_score(ytrue, yhat), recall_score(ytrue, yhat)])\n",
    "recall_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time v1\n",
    "Predict only one combination at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_video = []\n",
    "sentence = ['','']\n",
    "predictions = []\n",
    "\n",
    "num_frames_for_stability = 10\n",
    "threshold = 0.75\n",
    "\n",
    "# Define known combinations and their corresponding suggested next gestures\n",
    "known_combinations = {\n",
    "    '123': ['1', '2', '3'],\n",
    "    '212': ['2', '1', '2'],\n",
    "    '312': ['3', '1', '2'],\n",
    "    # Add more combinations and their suggested next gestures here\n",
    "}\n",
    "\n",
    "actual_combination = \"\"\n",
    "idx = 0\n",
    "\n",
    "# Initialize the suggested next gesture variable\n",
    "suggested_next_gesture = \"\"\n",
    "\n",
    "# Update the positions for displaying correct prediction and suggestion\n",
    "left_text_position = (120, 100)\n",
    "right_text_position = (120, 200)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(feedforward.CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = feedforward.mediapipe_detection(frame, holistic)\n",
    "\n",
    "        feedforward.draw_pose_landmarks(image, results)\n",
    "\n",
    "        keypoints = feedforward.extract_descriptor(results, DESCRIPTOR_LEN)\n",
    "        idx_video.append(keypoints)\n",
    "        idx_video = idx_video[-NUM_FRAMES_PER_VIDEO:]\n",
    "\n",
    "        if len(idx_video) == NUM_FRAMES_PER_VIDEO:\n",
    "            res = model.predict(np.expand_dims(idx_video, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-num_frames_for_stability:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                if res[np.argmax(res)] > threshold:            \n",
    "                    if len(sentence) > 0: \n",
    "                        if ACTIONS[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(ACTIONS[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(ACTIONS[np.argmax(res)])\n",
    "                    \n",
    "                    #Case 1: If the actual combination is empty, then check if the last gesture is the start of a known combination\n",
    "                    if actual_combination == \"\":\n",
    "                        for combination in known_combinations:\n",
    "                            if combination.startswith(''.join(sentence[-1])):\n",
    "                                actual_combination = combination\n",
    "                                idx = 1\n",
    "                                suggested_next_gesture = known_combinations[combination][idx]\n",
    "                                break\n",
    "                    else:\n",
    "                        #If the actual combination is not empty:\n",
    "                        #Case 2: If the idx is ok and the last gesture is the next gesture of the actual combination\n",
    "                        if idx != 0 and idx < len(known_combinations[actual_combination]) and sentence[-1] == known_combinations[actual_combination][idx]:\n",
    "                            idx += 1\n",
    "                            if idx < len(known_combinations[actual_combination]):   \n",
    "                                suggested_next_gesture = known_combinations[actual_combination][idx]\n",
    "                            print('next', idx)\n",
    "                        #Case 3: If the idx is ok and the last gesture is still the same as the previous one\n",
    "                        elif idx != 0 and idx < len(known_combinations[actual_combination]) and sentence[-1] == known_combinations[actual_combination][idx-1]:\n",
    "                            suggested_next_gesture = known_combinations[actual_combination][idx]\n",
    "                            print('still', idx)\n",
    "                        #Case 4: If the idx is over the length of the actual combination, then combination is done\n",
    "                        elif idx >= len(known_combinations[actual_combination]):\n",
    "                            print('done', idx)\n",
    "                            idx = 0\n",
    "                            actual_combination = \"\"\n",
    "                            suggested_next_gesture = \"done\"\n",
    "                        #Case 5: If the last gesture is not the next gesture of the actual combination, then reset the combination\n",
    "                        else:\n",
    "                            print('reset', idx)\n",
    "                            idx = 0\n",
    "                            actual_combination = \"\"\n",
    "                            suggested_next_gesture = \"\"\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "\n",
    "            image = feedforward.prob_viz(res, ACTIONS, image)\n",
    "\n",
    "        # Display the correct prediction on the left\n",
    "        if sentence:\n",
    "            cv2.putText(image, f'Correct Prediction: {sentence[-1]}', left_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.putText(image, f'Next Gesture: {suggested_next_gesture}', right_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time v2\n",
    "Predict more combinations at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 18:57:23.236 Python[98837:8915323] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     flag \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(suggested_next_gesture)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(ACTIONS \u001b[39m==\u001b[39m suggested_next_gesture)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     open_suggested_gif(ACTIONS, \u001b[39mint\u001b[39;49m(index[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     terminate_all_threads()\n",
      "\u001b[1;32m/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Check if a GIF for this action is already running\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m actions[idx] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m GIF_THREADS \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m GIF_THREADS[actions[idx]]\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#thread = threading.Thread(target=display_gif, args=(gif_path, actions[idx]))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# TODO: \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     display_gif(gif_path, actions[idx])\n",
      "\u001b[1;32m/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb Cell 20\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(window_name, gif_frame)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Display time of each frame in milliseconds\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     cv2\u001b[39m.\u001b[39;49mwaitKey(display_time_ms)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vtkachenko/Documents/projects/3d-Gesture-Feedbacks-and-Feeedforwards/nb_next_gesture.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyWindow(window_name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx_video = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "num_frames_for_stability = 10\n",
    "threshold = 0.75\n",
    "\n",
    "# Define known combinations and their corresponding suggested next gestures\n",
    "known_combinations = {\n",
    "    '123': ['1', '2', '3'],\n",
    "    '212': ['2', '1', '2'],\n",
    "    '312': ['3', '1', '2'],\n",
    "    '3212': ['3', '2', '1', '2'],\n",
    "    '213': ['2', '1', '3'],\n",
    "    '2312': ['2', '3', '1', '2'],\n",
    "    # Add more combinations and their suggested next gestures here\n",
    "}\n",
    "\n",
    "actual_combination = \"\"\n",
    "idx = 0\n",
    "\n",
    "# Initialize the suggested next gesture variable\n",
    "suggested_next_gesture = \"\"\n",
    "next_gestures = {}\n",
    "\n",
    "# Update the positions for displaying correct prediction and suggestion\n",
    "#left_text_position = (100, 560)\n",
    "#right_text_position = (600, 560)\n",
    "left_text_position = (120, 400)\n",
    "right_text_position = (120, 300)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(feedforward.CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = feedforward.mediapipe_detection(frame, holistic)\n",
    "\n",
    "        feedforward.draw_pose_landmarks(image, results)\n",
    "\n",
    "        keypoints = feedforward.extract_descriptor(results, DESCRIPTOR_LEN)\n",
    "        idx_video.append(keypoints)\n",
    "        idx_video = idx_video[-NUM_FRAMES_PER_VIDEO:]\n",
    "\n",
    "        if len(idx_video) == NUM_FRAMES_PER_VIDEO:\n",
    "            res = model.predict(np.expand_dims(idx_video, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-num_frames_for_stability:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                if res[np.argmax(res)] > threshold:            \n",
    "                    if len(sentence) > 0: \n",
    "                        if ACTIONS[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(ACTIONS[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(ACTIONS[np.argmax(res)])\n",
    "                    \n",
    "                    #Case 1: If the actual combination is empty, then check if the last gesture is the start of a known combination\n",
    "                    if next_gestures == {}:\n",
    "                        for combination in known_combinations:\n",
    "                            if combination.startswith(''.join(sentence[-1])):\n",
    "                                idx = 1\n",
    "                                next_gestures[combination] = idx\n",
    "                    #If the actual combination is not empty:\n",
    "                    else:\n",
    "                        for combo in list(next_gestures.keys()):\n",
    "                            idx = next_gestures[combo]\n",
    "                            #Case 2: If the idx is ok and the last gesture is the next gesture of the actual combination\n",
    "                            if idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx]:\n",
    "                                idx += 1\n",
    "                                next_gestures[combo] = idx\n",
    "                            #Case 3: If the idx is ok and the last gesture is still the same as the previous one\n",
    "                            elif idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx-1]:\n",
    "                                next_gestures[combo] = idx\n",
    "                            #Case 4: If the idx is over the length of the actual combination, then combination is done\n",
    "                            elif idx >= len(known_combinations[combo]):\n",
    "                                next_gestures.pop(combo)\n",
    "                            #Case 5: If the last gesture is not the next gesture of the actual combination, then reset the combination\n",
    "                            elif sentence[-1] != known_combinations[combo][idx]:\n",
    "                                next_gestures.pop(combo)\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "\n",
    "            image = feedforward.prob_viz(res, ACTIONS, image)\n",
    "\n",
    "        # Display the correct prediction on the left\n",
    "        if sentence:\n",
    "            cv2.putText(image, f'Correct Prediction: {sentence[-1]}', left_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        offset = 0\n",
    "        \n",
    "        for combo in next_gestures:\n",
    "            if next_gestures[combo] < len(known_combinations[combo]):\n",
    "                suggested_next_gesture = known_combinations[combo][next_gestures[combo]]\n",
    "            else: \n",
    "                suggested_next_gesture = \"done\"\n",
    "\n",
    "            cv2.putText(image, f'{known_combinations[combo]} Next Gesture: {suggested_next_gesture}', right_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            try:\n",
    "                flag = int(suggested_next_gesture)\n",
    "                index = np.where(ACTIONS == suggested_next_gesture)\n",
    "                open_suggested_gif(ACTIONS, int(index[0][0]))\n",
    "            except ValueError:\n",
    "                terminate_all_threads()\n",
    "\n",
    "            offset += 30\n",
    "            right_text_position = (120, 200 + offset)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            terminate_all_threads()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
