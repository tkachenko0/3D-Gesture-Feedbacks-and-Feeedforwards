{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import imageio\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "import feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMPY_ARRAYS_PATH = os.path.join('dist/numpy_arrays')\n",
    "VIDEOS_PATH = os.path.join('dist/videos')\n",
    "GIF_THREADS = {}\n",
    "ACTIONS = np.array(['1', '2', '3'])\n",
    "NUM_VIDEOS_PER_ACTION = 20\n",
    "NUM_FRAMES_PER_VIDEO = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Videos Dataset, extract Numpy Arrays and create Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward.create_folder_if_not_exists(NUMPY_ARRAYS_PATH)\n",
    "feedforward.create_folder_if_not_exists(VIDEOS_PATH)\n",
    "\n",
    "for action in ACTIONS:\n",
    "    feedforward.create_folder_if_not_exists(os.path.join(NUMPY_ARRAYS_PATH, action))\n",
    "    feedforward.create_folder_if_not_exists(os.path.join(VIDEOS_PATH, action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in ACTIONS:\n",
    "    video_prefix = ''\n",
    "    \n",
    "    subdir_videos = os.path.join(VIDEOS_PATH, action)\n",
    "    feedforward.register_videos(subdir_videos, video_prefix, NUM_VIDEOS_PER_ACTION, NUM_FRAMES_PER_VIDEO, action)\n",
    "    \n",
    "    subdir_numpy_arrays = os.path.join(NUMPY_ARRAYS_PATH, action)\n",
    "    feedforward.videos_2_numpy_pose_arrays(subdir_videos, subdir_numpy_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gif(gif_path: str, window_name: str, display_time_ms: int = 100):\n",
    "    gif = imageio.get_reader(gif_path)\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    for gif_frame in gif:\n",
    "        cv2.imshow(window_name, gif_frame)\n",
    "        cv2.waitKey(display_time_ms)\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "def terminate_all_threads():\n",
    "    global GIF_THREADS\n",
    "    for action, thread in GIF_THREADS.items():\n",
    "        thread.join()\n",
    "    GIF_THREADS = {} \n",
    "\n",
    "def open_suggested_gif(gif_path):\n",
    "    if gif_path not in GIF_THREADS or not GIF_THREADS[gif_path].is_alive():\n",
    "        thread = threading.Thread(target=display_gif, args=(gif_path, gif_path))\n",
    "        thread.start()\n",
    "        GIF_THREADS[gif_path] = thread\n",
    "\n",
    "def save_gif(landmarks_list: list, output_path: str):\n",
    "    fig = plt.figure()\n",
    "    ani = animation.FuncAnimation(fig, partial(feedforward.draw_frame, landmarks_list=landmarks_list), frames=len(landmarks_list), interval=100)\n",
    "    ani.save(output_path, writer='imagemagick', fps=10) # TODO: Capire se si puo usare la variabile NUM_FRAMES_PER_VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in ACTIONS:\n",
    "    first_video_path = os.path.join(VIDEOS_PATH, action, '2.mp4')\n",
    "    pose_landmarks = feedforward.load_pose_landmarks_from_video(first_video_path)\n",
    "    output_path = os.path.join(NUMPY_ARRAYS_PATH, action, f'{action}.gif')\n",
    "    save_gif(pose_landmarks, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(ACTIONS)}\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in ACTIONS:\n",
    "    for idx_video in range(NUM_VIDEOS_PER_ACTION):\n",
    "        window = []\n",
    "        for frame_num in range(NUM_FRAMES_PER_VIDEO):\n",
    "            res = np.load(os.path.join(NUMPY_ARRAYS_PATH, action, str(idx_video), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(NUM_FRAMES_PER_VIDEO, feedforward.POSE_DESCRIPTOR_LEN)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(ACTIONS.shape[0], activation='softmax'))\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)\n",
    "precision_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(yhat)\n",
    "recall_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(ytrue)\n",
    "f_score = stats.hmean([precision_score(ytrue, yhat), recall_score(ytrue, yhat)])\n",
    "recall_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time v2\n",
    "Predict more combinations at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "NUM_FRAMES_FOR_STABILITY = 9\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "known_combinations = {\n",
    "    '123': ['1', '2', '3'],\n",
    "    '212': ['2', '1', '2'],\n",
    "    '312': ['3', '1', '2'],\n",
    "    #'3212': ['3', '2', '1', '2'],\n",
    "    #'213': ['2', '1', '3'],\n",
    "    #'2312': ['2', '3', '1', '2'],\n",
    "}\n",
    "\n",
    "actual_combination = \"\"\n",
    "\n",
    "suggested_next_gesture = \"\"\n",
    "next_gestures = {}\n",
    "\n",
    "DONE = \"DONE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stable_prediction(res):\n",
    "    if res[np.argmax(res)] > THRESHOLD:            \n",
    "        if len(sentence) > 0: \n",
    "            if ACTIONS[np.argmax(res)] != sentence[-1]:\n",
    "                sentence.append(ACTIONS[np.argmax(res)])\n",
    "        else:\n",
    "            sentence.append(ACTIONS[np.argmax(res)])\n",
    "        \n",
    "        #Case 1: If the actual combination is empty, then check if the last gesture is the start of a known combination\n",
    "        if next_gestures == {}:\n",
    "            for combination in known_combinations:\n",
    "                if combination.startswith(''.join(sentence[-1])):\n",
    "                    idx = 1\n",
    "                    next_gestures[combination] = idx\n",
    "        #If the actual combination is not empty:\n",
    "        else:\n",
    "            for combo in list(next_gestures.keys()):\n",
    "                idx = next_gestures[combo]\n",
    "                #Case 2: If the idx is ok and the last gesture is the next gesture of the actual combination\n",
    "                if idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx]:\n",
    "                    idx += 1\n",
    "                    next_gestures[combo] = idx\n",
    "                #Case 3: If the idx is ok and the last gesture is still the same as the previous one\n",
    "                elif idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx-1]:\n",
    "                    next_gestures[combo] = idx\n",
    "                #Case 4: If the idx is over the length of the actual combination, then combination is done\n",
    "                elif idx >= len(known_combinations[combo]):\n",
    "                    next_gestures.pop(combo)\n",
    "                #Case 5: If the last gesture is not the next gesture of the actual combination, then reset the combination\n",
    "                elif sentence[-1] != known_combinations[combo][idx]:\n",
    "                    next_gestures.pop(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(image):\n",
    "    left_text_position = (120, 400)\n",
    "    right_text_position = (120, 300)\n",
    "    \n",
    "    if sentence:\n",
    "        cv2.putText(image, f'Current Prediction: {sentence[-1]}', left_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        offset = 0\n",
    "        \n",
    "        for combo in next_gestures:\n",
    "            if next_gestures[combo] < len(known_combinations[combo]):\n",
    "                suggested_next_gesture = known_combinations[combo][next_gestures[combo]]\n",
    "            else: \n",
    "                suggested_next_gesture = DONE\n",
    "                next_gestures == {}\n",
    "\n",
    "            cv2.putText(image, f'{known_combinations[combo]} Next Gesture: {suggested_next_gesture}', right_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            if suggested_next_gesture == DONE:\n",
    "                next_gestures == {}\n",
    "                terminate_all_threads()\n",
    "            else:\n",
    "                index_gif = int(np.where(ACTIONS == suggested_next_gesture)[0][0])\n",
    "                gif_path = os.path.join(NUMPY_ARRAYS_PATH, ACTIONS[index_gif], f'{ACTIONS[index_gif]}.gif')\n",
    "                open_suggested_gif(gif_path)\n",
    "\n",
    "            offset += 30\n",
    "            right_text_position = (120, 200 + offset)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(feedforward.CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = feedforward.mediapipe_detection(frame, holistic)\n",
    "\n",
    "        feedforward.draw_pose_landmarks(image, results)\n",
    "\n",
    "        keypoints = feedforward.extract_pose_descriptor(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-NUM_FRAMES_PER_VIDEO:]\n",
    "\n",
    "        if len(sequence) == NUM_FRAMES_PER_VIDEO:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-NUM_FRAMES_FOR_STABILITY:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                process_stable_prediction(res)\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "            image = feedforward.prob_viz(res, ACTIONS, image)\n",
    "\n",
    "        display_results(image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            terminate_all_threads()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
