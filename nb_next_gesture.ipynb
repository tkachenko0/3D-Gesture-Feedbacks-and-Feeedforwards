{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import imageio\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "from jproperties import Properties\n",
    "\n",
    "gif_threads = {}\n",
    "\n",
    "CAMERA_INDEX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp.solutions.holistic.POSE_CONNECTIONS,\n",
    "                             mp.solutions.drawing_utils.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp.solutions.drawing_utils.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "\n",
    "DESCRIPTOR_LEN = len(mp.solutions.holistic.PoseLandmark) * 4\n",
    "\n",
    "def extract_descriptor(results):\n",
    "    return np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(DESCRIPTOR_LEN)\n",
    "\n",
    "def display_gif(gif_path, window_name, display_time_ms=100):\n",
    "    gif = imageio.get_reader(gif_path)\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    for i, gif_frame in enumerate(gif):\n",
    "        cv2.imshow(window_name, gif_frame)\n",
    "        cv2.waitKey(display_time_ms)  # Display time of each frame in milliseconds\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "# Function to run display_gif in a thread\n",
    "def display_gif_thread(gif_path, window_name):\n",
    "    display_gif(gif_path, window_name)\n",
    "\n",
    "def draw_frame(frame_index, landmarks_list):\n",
    "    plt.clf()\n",
    "    body_landmarks = landmarks_list[frame_index]\n",
    "\n",
    "    x = [lm.x for lm in body_landmarks.landmark]\n",
    "    y = [lm.y for lm in body_landmarks.landmark]\n",
    "\n",
    "    plt.scatter(x, y, s=20, c='blue')\n",
    "\n",
    "    connections = [(11, 12), (12, 24), (24, 23), (23, 11),\n",
    "                   (11, 13), (13, 15), (15, 17),\n",
    "                   (12, 14), (14, 16), (16, 18),\n",
    "                   (11, 25), (25, 23),\n",
    "                   (24, 26), (26, 28), (28, 30),\n",
    "                   (23, 27), (27, 29), (29, 31)]\n",
    "\n",
    "    for connection in connections:\n",
    "        plt.plot([x[connection[0]], x[connection[1]]], [y[connection[0]], y[connection[1]]], c='blue')\n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_match(known_seq, input_seq):\n",
    "    # Check if the input sequence matches the known sequence\n",
    "    return input_seq[-len(known_seq):] == known_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = os.path.join('Dataset_atomic_gestures')\n",
    "actions = np.array(['1', '2', '3'])\n",
    "num_sequences = 20\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, frame):\n",
    "    colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    for idx, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60 + idx * 40), (int(prob * 100), 90 + idx * 40), colors[idx], -1)\n",
    "        cv2.putText(output_frame, actions[idx], (0, 85 + idx * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        ##open gif of the current gesture if prob > 90%\n",
    "        #print(actions[idx], prob)\n",
    "        #if prob > 0.9:\n",
    "        #    gif_path = os.path.join(DATA_PATH, actions[idx], f'{actions[idx]}.gif')\n",
    "            # Check if a GIF for this action is already running\n",
    "        #    if actions[idx] not in gif_threads or not gif_threads[actions[idx]].is_alive():\n",
    "        #        thread = threading.Thread(target=display_gif_thread, args=(gif_path, actions[idx]))\n",
    "        #        thread.start()\n",
    "        #        gif_threads[actions[idx]] = thread\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_all_threads():\n",
    "    global gif_threads\n",
    "    for action, thread in gif_threads.items():\n",
    "        thread.join()\n",
    "    gif_threads = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_suggested_gif(actions, frame, idx):\n",
    "\n",
    "    gif_path = os.path.join(DATA_PATH, actions[idx], f'{actions[idx]}.gif')\n",
    "    # Check if a GIF for this action is already running\n",
    "    if actions[idx] not in gif_threads or not gif_threads[actions[idx]].is_alive():\n",
    "        thread = threading.Thread(target=display_gif_thread, args=(gif_path, actions[idx]))\n",
    "        thread.start()\n",
    "        gif_threads[actions[idx]] = thread\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear dist \n",
    "for file in os.listdir(DATA_PATH):\n",
    "    os.remove(os.path.join(DATA_PATH, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "for action in actions:\n",
    "    if not os.path.exists(os.path.join(DATA_PATH, action)):\n",
    "        os.mkdir(os.path.join(DATA_PATH, action))\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(landmarks_list, action):\n",
    "    fig = plt.figure()\n",
    "    ani = animation.FuncAnimation(fig, partial(draw_frame, landmarks_list=landmarks_list), frames=len(landmarks_list), interval=100)\n",
    "    output_gif = os.path.join(DATA_PATH, action, f'{action}.gif')\n",
    "    ani.save(output_gif, writer='imagemagick', fps=10)\n",
    "    if os.path.exists(output_gif):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        gif_created = False\n",
    "        landmarks_list = []\n",
    "        single_gestures = {}\n",
    "        recognized_combinations = []\n",
    "        suggested_next_gestures = {}\n",
    "\n",
    "        for sequence in range(num_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                if not gif_created:\n",
    "                    body_landmarks = results.pose_landmarks\n",
    "                    if body_landmarks:\n",
    "                        landmarks_list.append(body_landmarks)\n",
    "\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Action: {action}. Video #{sequence}', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, f'Action: {action}. Video #{sequence}', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                keypoints = extract_descriptor(results)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not gif_created and landmarks_list:\n",
    "                gif_created = create_gif(landmarks_list, action)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length, DESCRIPTOR_LEN)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, callbacks=[tb_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)\n",
    "#Define precision_score\n",
    "precision_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(yhat)\n",
    "#define recall_score\n",
    "recall_score = lambda ytrue, yhat: np.sum([1 for yt, yp in zip(ytrue, yhat) if yt == yp]) / len(ytrue)\n",
    "#Calculate the f score\n",
    "f_score = stats.hmean([precision_score(ytrue, yhat), recall_score(ytrue, yhat)])\n",
    "f_score\n",
    "recall_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OLD) Test in Real Time v1\n",
    "Predict only one combination at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = ['','']\n",
    "predictions = []\n",
    "\n",
    "num_frames_for_stability = 10\n",
    "threshold = 0.75\n",
    "\n",
    "# Define known combinations and their corresponding suggested next gestures\n",
    "known_combinations = {\n",
    "    '123': ['1', '2', '3'],\n",
    "    '212': ['2', '1', '2'],\n",
    "    '312': ['3', '1', '2'],\n",
    "    # Add more combinations and their suggested next gestures here\n",
    "}\n",
    "\n",
    "actual_combination = \"\"\n",
    "idx = 0\n",
    "\n",
    "# Initialize the suggested next gesture variable\n",
    "suggested_next_gesture = \"\"\n",
    "\n",
    "# Update the positions for displaying correct prediction and suggestion\n",
    "left_text_position = (120, 100)\n",
    "right_text_position = (120, 200)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        keypoints = extract_descriptor(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-num_frames_for_stability:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                if res[np.argmax(res)] > threshold:            \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "                    #Case 1: If the actual combination is empty, then check if the last gesture is the start of a known combination\n",
    "                    if actual_combination == \"\":\n",
    "                        for combination in known_combinations:\n",
    "                            if combination.startswith(''.join(sentence[-1])):\n",
    "                                actual_combination = combination\n",
    "                                idx = 1\n",
    "                                suggested_next_gesture = known_combinations[combination][idx]\n",
    "                                break\n",
    "                    else:\n",
    "                        #If the actual combination is not empty:\n",
    "                        #Case 2: If the idx is ok and the last gesture is the next gesture of the actual combination\n",
    "                        if idx != 0 and idx < len(known_combinations[actual_combination]) and sentence[-1] == known_combinations[actual_combination][idx]:\n",
    "                            idx += 1\n",
    "                            if idx < len(known_combinations[actual_combination]):   \n",
    "                                suggested_next_gesture = known_combinations[actual_combination][idx]\n",
    "                            print('next', idx)\n",
    "                        #Case 3: If the idx is ok and the last gesture is still the same as the previous one\n",
    "                        elif idx != 0 and idx < len(known_combinations[actual_combination]) and sentence[-1] == known_combinations[actual_combination][idx-1]:\n",
    "                            suggested_next_gesture = known_combinations[actual_combination][idx]\n",
    "                            print('still', idx)\n",
    "                        #Case 4: If the idx is over the length of the actual combination, then combination is done\n",
    "                        elif idx >= len(known_combinations[actual_combination]):\n",
    "                            print('done', idx)\n",
    "                            idx = 0\n",
    "                            actual_combination = \"\"\n",
    "                            suggested_next_gesture = \"done\"\n",
    "                        #Case 5: If the last gesture is not the next gesture of the actual combination, then reset the combination\n",
    "                        else:\n",
    "                            print('reset', idx)\n",
    "                            idx = 0\n",
    "                            actual_combination = \"\"\n",
    "                            suggested_next_gesture = \"\"\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image)\n",
    "\n",
    "        # Display the correct prediction on the left\n",
    "        if sentence:\n",
    "            cv2.putText(image, f'Correct Prediction: {sentence[-1]}', left_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.putText(image, f'Next Gesture: {suggested_next_gesture}', right_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time v2\n",
    "Predict more combinations at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "num_frames_for_stability = 10\n",
    "threshold = 0.75\n",
    "\n",
    "# Define known combinations and their corresponding suggested next gestures\n",
    "known_combinations = {\n",
    "    '123': ['1', '2', '3'],\n",
    "    '212': ['2', '1', '2'],\n",
    "    '312': ['3', '1', '2'],\n",
    "    '3212': ['3', '2', '1', '2'],\n",
    "    '213': ['2', '1', '3'],\n",
    "    '2312': ['2', '3', '1', '2'],\n",
    "    # Add more combinations and their suggested next gestures here\n",
    "}\n",
    "\n",
    "actual_combination = \"\"\n",
    "idx = 0\n",
    "\n",
    "# Initialize the suggested next gesture variable\n",
    "suggested_next_gesture = \"\"\n",
    "next_gestures = {}\n",
    "\n",
    "# Update the positions for displaying correct prediction and suggestion\n",
    "#left_text_position = (100, 560)\n",
    "#right_text_position = (600, 560)\n",
    "left_text_position = (120, 400)\n",
    "right_text_position = (120, 300)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        keypoints = extract_descriptor(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "             \n",
    "            is_stable_prediction = np.unique(predictions[-num_frames_for_stability:])[0]==np.argmax(res)\n",
    "            if is_stable_prediction: \n",
    "                if res[np.argmax(res)] > threshold:            \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "                    #Case 1: If the actual combination is empty, then check if the last gesture is the start of a known combination\n",
    "                    if next_gestures == {}:\n",
    "                        for combination in known_combinations:\n",
    "                            if combination.startswith(''.join(sentence[-1])):\n",
    "                                idx = 1\n",
    "                                next_gestures[combination] = idx\n",
    "                    #If the actual combination is not empty:\n",
    "                    else:\n",
    "                        for combo in list(next_gestures.keys()):\n",
    "                            idx = next_gestures[combo]\n",
    "                            #Case 2: If the idx is ok and the last gesture is the next gesture of the actual combination\n",
    "                            if idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx]:\n",
    "                                idx += 1\n",
    "                                next_gestures[combo] = idx\n",
    "                            #Case 3: If the idx is ok and the last gesture is still the same as the previous one\n",
    "                            elif idx != 0 and idx < len(known_combinations[combo]) and sentence[-1] == known_combinations[combo][idx-1]:\n",
    "                                next_gestures[combo] = idx\n",
    "                            #Case 4: If the idx is over the length of the actual combination, then combination is done\n",
    "                            elif idx >= len(known_combinations[combo]):\n",
    "                                next_gestures.pop(combo)\n",
    "                            #Case 5: If the last gesture is not the next gesture of the actual combination, then reset the combination\n",
    "                            elif sentence[-1] != known_combinations[combo][idx]:\n",
    "                                next_gestures.pop(combo)\n",
    "\n",
    "            if len(sentence) > 5: sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(res, actions, image)\n",
    "\n",
    "        # Display the correct prediction on the left\n",
    "        if sentence:\n",
    "            cv2.putText(image, f'Correct Prediction: {sentence[-1]}', left_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        offset = 0\n",
    "        for combo in next_gestures:\n",
    "            if next_gestures[combo] < len(known_combinations[combo]):\n",
    "                suggested_next_gesture = known_combinations[combo][next_gestures[combo]]\n",
    "            else: \n",
    "                suggested_next_gesture = \"done\"\n",
    "            cv2.putText(image, f'{known_combinations[combo]} Next Gesture: {suggested_next_gesture}', right_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            try:\n",
    "                flag = int(suggested_next_gesture)\n",
    "                index = np.where(actions == suggested_next_gesture)\n",
    "                open_suggested_gif(actions, image, int(index[0][0]))\n",
    "            except ValueError:\n",
    "                terminate_all_threads()\n",
    "            offset += 30\n",
    "            right_text_position = (120, 200 + offset)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            terminate_all_threads()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
